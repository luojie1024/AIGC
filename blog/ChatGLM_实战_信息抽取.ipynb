{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 下载代码"
      ],
      "metadata": {
        "id": "Tb-uH8mjjLpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM-6B.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjebSELBBE5s",
        "outputId": "e3b3078e-6795-4b51-d7ee-3db957ca2f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM-6B'...\n",
            "remote: Enumerating objects: 1051, done.\u001b[K\n",
            "remote: Counting objects: 100% (1051/1051), done.\u001b[K\n",
            "remote: Compressing objects: 100% (423/423), done.\u001b[K\n",
            "remote: Total 1051 (delta 623), reused 1014 (delta 616), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1051/1051), 7.89 MiB | 22.49 MiB/s, done.\n",
            "Resolving deltas: 100% (623/623), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 安装环境"
      ],
      "metadata": {
        "id": "PnnSIWgvjPJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r /content/ChatGLM-6B/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kNceOOhBJqU",
        "outputId": "75f099c2-dade-4bc4-f5be-2f7afddb7fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r /content/ChatGLM-6B/requirements.txt (line 1)) (3.20.3)\n",
            "Requirement already satisfied: transformers==4.27.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/ChatGLM-6B/requirements.txt (line 2)) (4.27.1)\n",
            "Collecting cpm_kernels (from -r /content/ChatGLM-6B/requirements.txt (line 3))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from -r /content/ChatGLM-6B/requirements.txt (line 4)) (2.0.0+cu118)\n",
            "Collecting gradio (from -r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading gradio-3.28.1-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdtex2html (from -r /content/ChatGLM-6B/requirements.txt (line 6))\n",
            "  Downloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting sentencepiece (from -r /content/ChatGLM-6B/requirements.txt (line 7))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r /content/ChatGLM-6B/requirements.txt (line 8))\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (16.0.2)\n",
            "Collecting aiofiles (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting aiohttp (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.2.2)\n",
            "Collecting fastapi (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.1.3 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading gradio_client-0.1.4-py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/286.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading orjson-3.8.11-cp310-cp310-manylinux_2_28_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.10.7)\n",
            "Collecting pydub (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading websockets-11.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mdtex2html->-r /content/ChatGLM-6B/requirements.txt (line 6)) (3.4.3)\n",
            "Collecting latex2mathml (from mdtex2html->-r /content/ChatGLM-6B/requirements.txt (line 6))\n",
            "  Downloading latex2mathml-3.75.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r /content/ChatGLM-6B/requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.1.3->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2023.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading linkify_it_py-2.0.1-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette<0.27.0,>=0.26.1 (from fastapi->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=ba8024bea1f97528e63f2a6c421285b420cf59f400851938317fefa6d980e387\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, ffmpy, cpm_kernels, websockets, uc-micro-py, semantic-version, python-multipart, orjson, multidict, latex2mathml, h11, frozenlist, async-timeout, aiofiles, yarl, uvicorn, starlette, mdtex2html, mdit-py-plugins, linkify-it-py, httpcore, aiosignal, httpx, fastapi, aiohttp, gradio-client, gradio, accelerate\n",
            "Successfully installed accelerate-0.18.0 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 cpm_kernels-1.0.11 fastapi-0.95.1 ffmpy-0.3.0 frozenlist-1.3.3 gradio-3.28.1 gradio-client-0.1.4 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 latex2mathml-3.75.3 linkify-it-py-2.0.1 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 multidict-6.0.4 orjson-3.8.11 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.26.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.2 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. 查看显卡"
      ],
      "metadata": {
        "id": "ca5dEYuijThm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu_2tp1rux6u",
        "outputId": "e4a92d66-c47a-4fe8-c42a-3a47ddd6be04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue May  2 07:33:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    26W /  70W |   4753MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. 加载模型"
      ],
      "metadata": {
        "id": "EzRh32q4jXMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\",trust_remote_code=True).half().cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcMb1-VLuJtC",
        "outputId": "354c767c-233d-4de9-e0ff-4e9904752435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/63d66b0572d11cedd5574b38da720299599539b3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/63d66b0572d11cedd5574b38da720299599539b3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/63d66b0572d11cedd5574b38da720299599539b3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/63d66b0572d11cedd5574b38da720299599539b3/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. 模型推理"
      ],
      "metadata": {
        "id": "1d8vn7QzjbaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nah84IKIYpUH",
        "outputId": "142573f2-6118-4cff-b1b0-1d4721e4a2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "你好！请问有什么需要帮助的吗？\n",
            "CPU times: user 1.73 s, sys: 0 ns, total: 1.73 s\n",
            "Wall time: 1.89 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. 信息抽取任务"
      ],
      "metadata": {
        "id": "b-4-T6obZUWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "content=\"\"\"ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，\n",
        "基于 General Language Model (GLM) 架构，具有 62 亿参数。\n",
        "手机号 18866666666\n",
        "结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 \n",
        "ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。\n",
        "邮箱 550997728@qq.com\n",
        "经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，\n",
        "账号 root 密码 safhahuifbt\n",
        "62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答，更多信息请参考我们的博客。\n",
        "\"\"\"\n",
        "prompt='从上文中，提取\"信息\"(keyword,content)，包括:\"手机号\"、\"邮箱\"、\"账号\"、\"密码\"等类型的实体，输出json格式内容'\n",
        "input ='{}\\n\\n{}'.format(content,prompt)\n",
        "print(input)\n",
        "response, history = model.chat(tokenizer, input, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94MmRC3pV6ke",
        "outputId": "39d76400-ac5e-4644-f2e1-b198f7eb35ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，\n",
            "基于 General Language Model (GLM) 架构，具有 62 亿参数。\n",
            "手机号 18866666666\n",
            "结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 \n",
            "ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。\n",
            "邮箱 550997728@qq.com\n",
            "经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，\n",
            "账号 root 密码 safhahuifbt\n",
            "62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答，更多信息请参考我们的博客。\n",
            "\n",
            "\n",
            "从上文中，提取\"信息\"(keyword,content)，包括:\"手机号\"、\"邮箱\"、\"账号\"、\"密码\"等类型的实体，输出json格式内容\n",
            "{\n",
            "  \"keyword\": \"信息\",\n",
            "  \"content\": {\n",
            "    \"手机号\": \"18866666666\",\n",
            "    \"邮箱\": \"550997728@qq.com\",\n",
            "    \"账号\": \"root\",\n",
            "    \"密码\": \"safhahuifbt\"\n",
            "  }\n",
            "}\n",
            "CPU times: user 14.8 s, sys: 31.5 ms, total: 14.9 s\n",
            "Wall time: 15.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. 结构化输出校验"
      ],
      "metadata": {
        "id": "xY06Zt_ZjpzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "json.loads(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOHSrUHKd-SI",
        "outputId": "e3cd819a-5626-4829-9681-6d798a5e7c0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'keyword': '信息',\n",
              " 'content': {'手机号': '18866666666',\n",
              "  '邮箱': '550997728@qq.com',\n",
              "  '账号': 'root',\n",
              "  '密码': 'safhahuifbt'}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXSWyHL5kUIX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}